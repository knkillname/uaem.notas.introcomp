# 9. Introducción a la Algoritmia

- *Autor*: [Dr. Mario Abarca](https://www.knkillname.org/)
- *Objetivos*: Comprender los conceptos fundamentales de problemas computacionales y algoritmos; calcular y analizar la complejidad temporal de algoritmos; conocer estrategias de diseño de algoritmos.

Hasta ahora el curso se ha centrado en escribir algoritmos en Python para resolver problemas.
Sin embargo, no hemos discutido cómo medir formalmente (a priori) los recursos que consume un algoritmo.

## 9.1 Problemas computacionales

**Definición** (De Relación): Una **relación** entre dos conjuntos $A$ y $B$ es un subconjunto $R \subseteq A \times B$.
Decimos que $a$ está relacionado con $b$ si $(a, b) \in R$.

**Ejemplo**: El juego de *piedra, papel o tijera* es una relación entre los conjuntos $A = \{\mathit{piedra}, \mathit{papel}, \mathit{tijera}\}$ y sí mismo. La relación $\mathit{vence}(a, b)$ indica si el elemento $a$ vence al elemento $b$, y está definida como:

$$\mathit{VENCE} = \{(\mathit{piedra}, \mathit{tijera}), (\mathit{papel}, \mathit{piedra}), (\mathit{tijera}, \mathit{papel})\}$$

La notación conjuntista no siempre es la más clara para representar relaciones, por lo que a menudo se utilizan otras notaciones como la notación de predicados o la notación de relaciones que son más intuitivas:

- En la *notación de conjuntos*: $(a, b) \in R$
- En la *notación de relación*: $a\,R\,b$
- En la *notación de predicado*: $R(a, b)$

Continuando con el ejemplo de *piedra, papel o tijera*, la relación $\mathit{VENCE}$ puede representarse de las siguientes maneras:

| Notación de conjuntos                                   | Relación                                           | Predicado                                          |
| ------------------------------------------------------- | -------------------------------------------------- | -------------------------------------------------- |
| $(\mathit{piedra}, \mathit{tijera}) \in \mathit{VENCE}$ | $\mathit{piedra}\,\mathit{VENCE}\,\mathit{tijera}$ | $\mathit{VENCE}(\mathit{piedra}, \mathit{tijera})$ |
| $(\mathit{papel}, \mathit{piedra}) \in \mathit{VENCE}$  | $\mathit{papel}\,\mathit{VENCE}\,\mathit{piedra}$  | $\mathit{VENCE}(\mathit{papel}, \mathit{piedra})$  |
| $(\mathit{tijera}, \mathit{papel}) \in \mathit{VENCE}$  | $\mathit{tijera}\,\mathit{VENCE}\,\mathit{papel}$  | $\mathit{VENCE}(\mathit{tijera}, \mathit{papel})$  |

Una manera alternativa de representar la relación es como una tabla de verdad, donde cada fila representa una relación entre dos elementos.

| $\mathit{VENCE}$  | $\mathit{piedra}$ | $\mathit{papel}$ | $\mathit{tijera}$ |
| ----------------- | ----------------- | ---------------- | ----------------- |
| $\mathit{piedra}$ | 0                 | 0                | 1                 |
| $\mathit{papel}$  | 1                 | 0                | 0                 |
| $\mathit{tijera}$ | 0                 | 1                | 0                 |

Hay muchos tipos de problemas, como cuando te peleas con tu suegra, o cuando se te cae el café en la computadora, pero para este curso nos centraremos en los problemas computacionales.
Para ello, presentamos primero un ejemplo de física clásica:

**Ejemplo** (de problema formal): Se desea determinar cuánto tiempo tarda una bola de acero de 1 kg en caer desde una altura de 10 m.
Para resolver este problema, se puede utilizar la ecuación de movimiento uniformemente acelerado:
$$
h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2
$$
donde $h(t)$ es la altura en función del tiempo, $h_0$ es la altura inicial (10 m), $v_0$ es la velocidad inicial (0 m/s), $g$ es la aceleración debida a la gravedad (9.81 m/s²) y $t$ es el tiempo en segundos.
Para resolver la ecuación, se puede despejar $t$ y obtener:
$$
t = \sqrt{\frac{2h_0}{g}} \approx 1.43 \text{ s}
$$

Es importante dar un paso atrás y diseccionar este problema en sus partes fundamentales:

- *Datos del problema*: $h_0 = 10$ m, $v_0 = 0$ m/s, $g = 9.81$ m/s²
- *Datos de la solución*: $t \approx 1.43$ s
- *Relación entre los datos*: $h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2$

Más formalmente la relación entre los datos del problema y los datos de la solución se puede expresar como una *relación* entre los conjuntos:

- *Datos del problema*: $D_p = \{(h_0, v_0, g) \mid h_0, v_0, g \in \mathbf{R}\}$
- *Datos de la solución* $D_s = \{t \mid t \in \mathbf{R}\}$
- *Relación entre los datos* $R \subseteq D_p \times D_s$ definida como:

$$
R = \left\{(\vec d, t) \middle | \vec d = (h_0, v_0, g) \wedge h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2\right\}
$$

En este ejemplo, $(10, 0, 9.81) \in D_p$ y $1.43 \in D_s$ son los datos del problema y de la solución respectivamente, y la relación es $(10, 0, 9.81)\,R\,1.43$.
Podemos generalizar este problema así:

**Problema** (de la caída libre):

- **Dados** $h_0 \in \mathbf{R}$, $v_0 \in \mathbf{R}$, $g \in \mathbf{R}$
- **encontrar** $t \in \mathbf{R}$ **tal que** $h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2$

Asimismo, la solución se generaliza como:

**Solución** (del problema de la caída libre):

- **Dados**: $h_0 \in \mathbf{R}$, $v_0 \in \mathbf{R}$, $g \in \mathbf{R}$
- **Calcular**: $t = \sqrt{\frac{2h_0}{g}}$
- **Devolver**: $t \in \mathbf{R}$

Generalizando este ejemplo arribamos a la siguiente definición:

**Definición** (Problema computacional): Un **problema computacional** es una relación $R \subseteq E \times S$ donde $E$ es el conjunto de **entradas** y $S$ es el conjunto de **salidas**.
La **solución** de un problema computacional es un algoritmo $A: E \to S$ tal que $A(e) = s$ si y solo si $R(e, s)$.

### Problemas computacionales clásicos

#### Buscar un elemento en una lista

Quizá el problema computacional más clásico es el de buscar un elemento en una lista.

- **Dados**: Una lista $L = [a_0, a_1, \dots, a_{n - 1}]$ y un elemento $x$.
- **Encontrar**: Un índice $i\in \{0,1,\ldots, n-1\}$ **tal que** $L[i] = x$ si es que existe, o $i = -1$ si no existe.

**Ejemplo**:

- Entrada: $L = [3, 5, 7, 9]$, $x = 7$
- Salida: $i = 2$

#### Ordenar una lista

El problema de ordenar una lista es uno de los problemas computacionales más clásicos y se puede formular de la siguiente manera:

- **Dados**: Una lista $L = [a_0, a_1, \dots, a_{n - 1}]$.
- **Encontrar**: Una lista $L' = [a'_0, a'_1, \dots, a'_{n - 1}]$ **tal que** $L'$ es una permutación de $L$ y $L'$ está ordenada.

**Ejemplo**:

- Entrada: $L = [8,5,6,9,0,2,1,3,4,7]$
- Salida: $L' = [0,1,2,3,4,5,6,7,8,9]$

#### El problema de la suma de subconjunto

Este problema es equivalente a devolver el cambio de un billete de $m$ usando monedas de denominaciones $c_0, c_1, \dots, c_{n - 1}$ posiblemente con repetición.

- **Dados**: Un monto $m \in \mathbf{N}$ y una lista de monedas $C = [c_0, c_1, \dots, c_{n - 1}]$.
- **Encontrar**: Una sublista $C' = [c'_0, c'_1, \dots, c'_{k - 1}]$ de $C$ **tal que** $c'_0 + c'_1 + \dots + c'_{k - 1} = m$.

**Ejemplo**:

- Entrada: $m = 7$, $C = [1, 2, 2, 5, 10]$
- Salida: $C' = [2, 5]$

#### El problema de las 8 reinas

Este es un ejemplo de un problema *finito* pero interesante.

- **Dados**: Un tablero de ajedrez de $8 \times 8$, así como 8 reinas de ajedrez.
- **Encontrar**: Una lista de posiciones $P = [(x_0, y_0), (x_1, y_1), \ldots, (x_7, y_7)]$ **tal que** al colocar las reinas en esas posiciones, ninguna reina puede atacar a otra.

### Actividades y ejercicios

1. **Identificar problemas computacionales**:
   - Visita [CodeSignal](https://codesignal.com/) y selecciona algunos problemas de programación.
   - Para cada problema, identifica:
     - Los datos de entrada.
     - Los datos de salida.
     - La relación entre los datos de entrada y salida.

2. **Clasificar problemas**:
   - Pregunta a ChatGPT o el asistente de tu elección sobre problemas de búsqueda, ordenamiento y optimización.
   - Clasifica los problemas seleccionados en categorías como búsqueda, ordenamiento, optimización, etc.
   - Reflexiona sobre las estrategias que podrías usar para resolverlos.

3. **Explorar problemas del mundo real**:
   - Piensa en un problema cotidiano que pueda modelarse como un problema computacional.
   - Define los datos de entrada, los datos de salida y la relación entre ellos.
   - Intenta formular un algoritmo para resolver el problema.

## 9.2 Análisis de algoritmos

En la práctica no sólo es importante resolver un problema computacional, sino que la solución se pueda obtener en un tiempo razonable, o que pueda ser ejecutado en un dispositivo con recursos limitados.
Antes de comprar esa computadora de última generación, primero averigua si no es más barato diseñar un algoritmo más eficiente.
Para ello, es importante analizar la complejidad de un algoritmo, que se refiere a la cantidad de recursos (tiempo y espacio) que consume al ejecutarse.

### Complejidad temporal

El número de pasos que un algoritmo requiere para resolver un problema no sólo depende del algoritmo en sí, sino también de la entrada que se le proporciona.
Por ejemplo, consideremos el algoritmo de búsqueda lineal:

```python
def busqueda_lineal(L, x):
    for i in range(len(L)):
        if L[i] == x:
            return i
    return -1
```

En este caso, el número de pasos que toma el algoritmo depende del tamaño de la lista $L$ y del valor de $x$.

- Si $L$ tiene $n$ elementos, el algoritmo tomará $n$ pasos en el peor de los casos (cuando $x$ no está en la lista o está al final).
- Si $x$ está al principio de la lista, el algoritmo tomará 1 paso.
- Si $x$ está en el medio, tomará aproximadamente $n/2$ pasos.
- Si $x$ no está en la lista, tomará $n$ pasos.

Sea $T(e)$ el número de pasos que toma el algoritmo para una entrada $e$, y sean $E_n = \mathbf R^n$ el conjunto de entradas de tamaño $n$, es decir, el conjunto de todas las listas que tienen $n$ elementos.
Deseamos definir una función $T: \mathbf N \to \mathbf N$ que nos permita calcular el número de pasos que toma el algoritmo para una entrada de tamaño $n$.
Para ello tenemos tres principales enfoques:

- **Peor caso**: Es la cantidad máxima de pasos que toma el algoritmo para una entrada de tamaño $n$.
  $$T(n) = \max_{e \in E_n} T(e)$$
- **Mejor caso**: Es la cantidad mínima de pasos que toma el algoritmo para una entrada de tamaño $n$.
  $$T(n) = \min_{e \in E_n} T(e)$$
- **Caso promedio**: Es la cantidad promedio de pasos que toma el algoritmo para una entrada de tamaño $n$.
  Esto a su vez depende de una distribución de probabilidad sobre el conjunto de entradas:
  Suponiendo que $p(e)$ es la probabilidad de que la entrada $e$ ocurra, el caso promedio se define como:
  $$T(n) = \sum_{e \in E_n} p(e)\,T(e)$$

A menos que se indique lo contrario, siempre asume que el análisis de un algoritmo se refiere al peor caso: los computólogos son pesimistas por naturaleza.

#### Ejemplo: Ordenamiento por selección

El algoritmo de ordenamiento por selección funciona seleccionando repetidamente el elemento más pequeño de la lista no ordenada y colocándolo en su posición correcta.

```python
def seleccion(L):
    n = len(L) 
    for i in range(n - 1):
        # Sup. que el elemento más pequeño está en la posición i
        min_idx = i
    
        # Busca el elemento más pequeño del resto de la lista
        for j in range(i + 1, n):
            if L[j] < L[min_idx]:
                min_idx = j
        
        # Intercambia el elemento más pequeño encontrado con el elemento en la posición i
        L[i], L[min_idx] = L[min_idx], L[i]
```

Para analizar la complejidad temporal del algoritmo, contamos el número de comparaciones realizadas en el ciclo interno.

1. En la primera iteración del ciclo externo ($i = 0$), el ciclo interno realiza $n - 1$ comparaciones.
2. En la segunda iteración ($i = 1$), el ciclo interno realiza $n - 2$ comparaciones.
3. En general, en la $i$-ésima iteración, el ciclo interno realiza $n - i - 1$ comparaciones.

El número total de comparaciones $T(n)$ realizadas por el algoritmo es la suma de todas las comparaciones realizadas en cada iteración del ciclo externo:

$$
\begin{align*}
T(n) &= \sum_{i=0}^{n-2} (n - i - 1) \\
  &= \sum_{i=0}^{n-2} (n - 1 - i) \\
  &= \sum_{i=0}^{n-2} (n - 1) - \sum_{i=0}^{n-2} i \\
  &= (n - 1)(n - 1) - \frac{(n - 2)(n - 1)}{2} \\
  &= \frac{2(n - 1)(n - 1) - (n - 2)(n - 1)}{2} \\
  &= \frac{(n - 1)(2n - 2 - n + 2)}{2} \\
  &= \frac{(n - 1)(n)}{2}.
\end{align*}
$$

Por lo tanto, el número total de comparaciones es:

$$T(n) = \frac{n(n - 1)}{2}.$$

### Crecimiento asintótico

Quizás te preguntes por qué no se considera el número de intercambios realizados por el algoritmo, o bien el número total de pasos que toma el algoritmo, incluyendo cada comparación, cada suma, cada asignación, etc.
Por ahora nos limitaremos a decir que estas otras operaciones son irrelevantes cuando consideras lo siguiente:

- El número de pasos que toma el algoritmo es proporcional al número de comparaciones realizadas.
- El tiempo exacto que toma el algoritmo depende de la implementación y del hardware utilizado, pero la relación entre el número de pasos y el tiempo es constante.
- La diferencia entre el número exacto de pasos y el número de comparaciones debe ser a lo sumo un factor constante más un término despreciable cuando $n$ es grande.

Llevando esta idea un poco más lejos, podemos formalizar este concepto de que unos términos sean despreciables en comparación con otros usando la notación *asintótica*, mejor conocida como **notación O-grande**.

**Definición** (Notación O-grande): Sea $f(n)$ y $g(n)$ funciones de $\mathbf{N} \to \mathbf{R}$.
Decimos que $f(n)$ es $O(g(n))$ (“$f$ crece no más rápido que $g$”) si existe una constante $c > 0$ tal que $f(n) \leq c\,g(n)$ para todo $n$ salvo un número finito de casos.

Por razones históricas usamos la notación $f(n) = O(g(n))$, donde el signo de igualdad no es del todo correcto.
Una notación más apropiada podría ser $f \le_O g$, pero nadie la usa.

**Ejemplo**: Se desea decidir entre dos algoritmos, $A$ y $B$, que resuelven el mismo problema.

- El algoritmo $A$ tiene una complejidad temporal de $T_A(n) = 2\,n + 20$.
- El algoritmo $B$ tiene una complejidad temporal de $T_B(n) = n^2 + 10\,n + 5$.

La elección depende del tamaño de la entrada $n$.

- Si $n$ es pequeño, $T_B(n)$ es más pequeño que $T_A(n)$; por ejemplo, para $n = 1$, $T_A(1) = 22$ y $T_B(1) = 16$.
- Si $n$ es grande, $T_A(n)$ es más pequeño que $T_B(n)$; por ejemplo, para $n = 100$, $T_A(100) = 220$ y $T_B(100) = 10505$.

Claramente el algoritmo $A$ es mejor conforme $n$ crece, decimos que $A$ *escala mejor que* $B$.
La notación O-grande nos permite expresar esto de manera formal:

$$
T_A(n) = O(T_B(n)).
$$

Para mostrar que $T_A(n) = O(T_B(n))$, debemos encontrar una constante $c > 0$ tal que $T_A(n) \leq c\,T_B(n)$ para todo $n$ salvo un número finito de casos. En este caso, basta con tomar $c = 1$ y $n \geq 20$.

Algunas reglas útiles para la notación O-grande son:

- Las constantes multiplicativas pueden ser ignoradas: $O(c\,f(n)) = O(f(n))$.
- La suma de dos funciones es la función que crece más rápido: $O(f(n) + g(n)) = O(\max(f(n), g(n)))$.
- $n^a$ domina $n^b$ si $a > b$: por ejemplo, $O(n^2 + n) = O(n^2)$.
- Toda función exponencial crece más rápido que cualquier polinómica: $O(n^k) = O(2^n)$.
- De forma similar toda función polinómica crece más rápido que cualquier función logarítmica: $O(\log(n)) = O(n^k)$.

### Ejemplos adicionales de notación O-grande

1. **Simplificación de $3\,n^2\,\log n + 20$**:
   - La constante $20$ es despreciable en comparación con $3\,n^2\,\log n$ para valores grandes de $n$.
   - Por lo tanto, $3\,n^2\,\log n + 20 = O(n^2\,\log n)$.

2. **Comparación de $5\,n^3 + 2\,n^2$**:
   - El término $5\,n^3$ domina $2\,n^2$ para valores grandes de $n$.
   - Por lo tanto, $5\,n^3 + 2\,n^2 = O(n^3)$.

3. **Simplificación de $n^4 + n^3 + n^2 + n$**:
   - El término $n^4$ domina todos los demás términos para valores grandes de $n$.
   - Por lo tanto, $n^4 + n^3 + n^2 + n = O(n^4)$.

4. **Logaritmos en diferentes bases**:
   - Los logaritmos en diferentes bases son equivalentes hasta un factor constante.
   - Por ejemplo, $\log_2(n) = O(\log_{10}(n))$.

5. **Crecimiento exponencial frente a polinómico**:
   - Una función exponencial como $2^n$ crece más rápido que cualquier función polinómica como $n^k$.
   - Por ejemplo, $n^3 + 2^n = O(2^n)$.

6. **Crecimiento logarítmico frente a constante**:
   - Una función logarítmica como $\log(n)$ crece más rápido que una constante.
   - Por ejemplo, $5 + \log(n) = O(\log(n))$.

Estos ejemplos ilustran cómo simplificar expresiones y comparar el crecimiento de diferentes funciones utilizando la notación O-grande.

### Ejemplos de conteo de operaciones con notación O-grande

1. **Ciclo simple**:

   ```python
   for i in range(n):
       # Operación constante
       print(i)
   ```

   - Complejidad: $O(n)$
   - El ciclo se ejecuta $n$ veces, y cada iteración realiza una operación constante.

2. **Ciclo anidado**:

   ```python
   for i in range(m):
       for j in range(n):
           # Operación constante
           print(i, j)
   ```

   - Complejidad: $O(m \cdot n)$
   - El ciclo externo se ejecuta $m$ veces, y por cada iteración del ciclo externo, el ciclo interno se ejecuta $n$ veces.

3. **Ciclo anidado con dependencia**:

   ```python
   for i in range(n):
       for j in range(i):
           # Operación constante
           print(i, j)
   ```

   - Complejidad: $O(n^2)$
   - El ciclo externo se ejecuta $n$ veces, pero el ciclo interno se ejecuta $i$ veces en la $i$-ésima iteración. El número total de iteraciones es:
     $$
     \sum_{i=1}^{n} i = \frac{n(n-1)}{2} = O(n^2)
     $$

4. **Ciclo con crecimiento logarítmico**:

   ```python
   i = 1
   while i < n:
       # Operación constante
       print(i)
       i *= 2
   ```

   - Complejidad: $O(\log(n))$
   - El valor de $i$ se duplica en cada iteración, por lo que el ciclo se ejecuta aproximadamente $\log_2(n)$ veces.

5. **Ciclo triple anidado**:

   ```python
   for i in range(n):
       for j in range(n):
           for k in range(n):
               # Operación constante
               print(i, j, k)
   ```

   - Complejidad: $O(n^3)$
   - Cada ciclo anidado se ejecuta $n$ veces, resultando en $n \cdot n \cdot n = n^3$ iteraciones.

Estos ejemplos muestran cómo analizar y calcular la complejidad temporal de diferentes estructuras de bucles utilizando la notación O-grande.

## 9.3 Estrategia de la Fuerza bruta

La **fuerza bruta** es una estrategia de diseño de algoritmos que consiste en examinar sistemáticamente todas las posibles soluciones a un problema para encontrar la óptima o una que cumpla con ciertos criterios. Aunque a menudo es la más sencilla de concebir e implementar, generalmente es la menos eficiente para problemas de tamaño considerable.

La idea central es explorar todo el *espacio de búsqueda*, que es el conjunto de todas las posibles soluciones candidatas. Para muchos problemas, este espacio de búsqueda puede ser inmensamente grande, lo que hace que la fuerza bruta sea computacionalmente inviable.

### El problema de la mochila

Un ejemplo clásico donde se puede aplicar (y analizar la ineficiencia) de la fuerza bruta es el *Problema de la Mochila*:
Este es el que ocurre cuando tienes una mochila que quieres llenar con objetos de diferentes pesos y valores, y te gustaría llevar la mayor cantidad de valor posible sin exceder el peso máximo que puede soportar la mochila.

**Problema** (de la mochila):

- **Dadas**: Dos listas de enteros $w = [w_0, w_1, \ldots, w_{n - 1}]$ y $v = [v_0, v_1, \ldots, v_{n - 1}]$, y un entero $W$.
- **Encontrar**: Un subconjunto $S \subseteq \{0, 1, \ldots, n - 1\}$ **tal que** $\sum_{i \in S} w_i \leq W$ y $\sum_{i \in S} v_i$ es máximo.

**Ejemplo**  
Tenemos que completar un examen de matemáticas; cada pregunta tiene una puntuación asociada, así como una cantidad de tiempo estimado para resolverla. El examen tiene un tiempo máximo de $60$ minutos. No hay tiempo para resolver todas las preguntas, así que debes elegir cuáles resolver para maximizar tu puntuación. Las preguntas tienen los siguientes tiempos y puntuaciones:

| Pregunta | Tiempo (min) | Puntuación |
| -------- | ------------ | ---------- |
| 1        | 10           | 5          |
| 2        | 15           | 8          |
| 3        | 10           | 4          |
| 4        | 20           | 10         |
| 5        | 5            | 2          |
| 6        | 30           | 20         |
| 7        | 5            | 3          |
| 8        | 25           | 15         |

En este caso, la instancia del problema de la mochila se puede formular como:

- **Dados**: $w = [10, 15, 10, 20, 5, 30, 5, 25]$, $v = [5, 8, 4, 10, 2, 20, 3, 15]$, y $W = 60$.
- **Encontrar**: Un subconjunto $S \subseteq \{0, 1, \ldots, 6\}$ **tal que** $\sum_{i \in S} w_i \leq 60$ y $\sum_{i \in S} v_i$ es máximo.

Por ejemplo, si eliges las preguntas 1, 2, 4 y 5, el tiempo total es $10 + 15 + 20 + 5 = 50$ minutos y la puntuación total (suponiendo que tus respuestas son correctas) es $5 + 8 + 10 + 2 = 25$ puntos. Si eliges las preguntas 1, 2, 3 y 5, el tiempo total es $10 + 15 + 10 + 5 = 40$ minutos y la puntuación total es $5 + 8 + 4 + 2 = 19$ puntos.
La primera selección es mejor, ya que produce una puntuación total mayor dentro del tiempo permitido.

Así como en este ejemplo, el problema de la mochila se puede encontrar en muchos contextos, como la planificación de proyectos (donde cada tarea tiene un tiempo y un beneficio asociado), la selección de inversiones (donde cada inversión tiene un costo y un retorno esperado), o incluso en la vida cotidiana (como decidir qué llevar en una mochila para un viaje de campamento).

### Ejercicios

1. **Explorar variantes del problema de la mochila**:
   - Investiga y describe al menos dos variantes del problema de la mochila, como el problema de la mochila fraccional o el problema de la mochila 0-1.
   - Implementa una solución para una de estas variantes utilizando fuerza bruta.
   - Discute con un asistente de IA las diferencias en complejidad y aplicabilidad entre estas variantes.

2. **Comparar enfoques**:
   - Implementa el problema de la mochila utilizando fuerza bruta y un enfoque más eficiente, como programación dinámica (pide a tu asistente de IA que te ayude a implementar este enfoque).
   - Analiza y compara el tiempo de ejecución de ambos enfoques para diferentes tamaños de entrada.
   - Reflexiona sobre los casos en los que la fuerza bruta podría ser preferible.

3. **Buscar aplicaciones reales**:
   - Identifica un problema cotidiano que pueda modelarse como un problema de la mochila.
   - Define los datos de entrada, los datos de salida y la relación entre ellos.
   - Discute con un asistente de IA cómo optimizar la solución para este problema.

### Algoritmo de Fuerza Bruta para el Problema de la Mochila

Utilizando la fuerza bruta para resolver el problema de la mochila, examinamos cada posible subconjunto de elementos. El conjunto de todos los subconjuntos de un conjunto de $n$ elementos se conoce como el **conjunto potencia**. Si tenemos $n$ elementos, hay $2^n$ posibles subconjuntos (cada ítem puede estar dentro o fuera de la mochila independientemente).
El algoritmo de fuerza bruta sería el siguiente:

1. Generar todos los $2^n$ subconjuntos de elementos.
2. Para cada subconjunto generado:
   1. Calcular el peso total de los elementos en el subconjunto.
   2. Si el peso total no excede la capacidad $W$ de la mochila, calcular el valor total de los elementos en el subconjunto.
3. Mantener un registro del subconjunto que tenga el valor total máximo entre todos los subconjuntos válidos (cuyo peso no exceda $W$).

A continuación, se presenta un ejemplo de implementación en Python:

```python
from itertools import chain, combinations

def conjunto_potencia(iterable):
   elementos = list(iterable)
   yield from chain.from_iterable(combinations(elementos, r) for r in range(len(elementos) + 1))

def mochila_fuerza_bruta(capacidad, pesos, valores):
   cantidad_elementos = len(pesos)
   if cantidad_elementos != len(valores):
      raise ValueError("Las listas de pesos y valores deben tener la misma longitud.")

   mejor_valor, mejor_seleccion = 0, []

   # Generar todos los subconjuntos posibles usando conjunto_potencia
   for subconjunto in conjunto_potencia(range(cantidad_elementos)):
      peso_actual = sum(pesos[i] for i in subconjunto)
      valor_actual = sum(valores[i] for i in subconjunto)

      # Verificar si el subconjunto actual es válido y si mejora el mejor valor encontrado
      if peso_actual <= capacidad y valor_actual > mejor_valor:
         mejor_valor = valor_actual
         mejor_seleccion = subconjunto

   return mejor_valor, mejor_seleccion

# Ejemplo de uso:
capacidad = 10
pesos = [2, 3, 4, 5]
valores = [3, 7, 2, 9]
mejor_valor, mejor_seleccion = mochila_fuerza_bruta(capacidad, pesos, valores)
print(f"Mejor valor posible: {mejor_valor}")
print(f"Índices de los elementos seleccionados: {list(mejor_seleccion)}")
```

### Análisis de Complejidad para `mochila_fuerza_bruta`

La complejidad de la solución por fuerza bruta para el problema de la mochila es dominada por la generación y evaluación de todos los subconjuntos posibles. Como hay $2^n$ subconjuntos y para cada subconjunto se itera sobre los $n$ elementos para calcular el peso y el valor, la complejidad temporal es $O(n \cdot 2^n)$.

Como vimos en la sección 9.2, una complejidad exponencial como $O(2^n)$ crece muy rápidamente a medida que $n$ aumenta. Esto significa que la solución de fuerza bruta solo es viable para un número pequeño de elementos. Para un problema con, digamos, $n=40$ elementos, $2^{40}$ es un número enormemente grande, haciendo que el tiempo de cálculo sea prohibitivo en la práctica.

La complejidad espacial de esta aproximación es $O(n)$ para almacenar la mejor selección encontrada hasta el momento.

Debido a su ineficiencia para problemas de tamaño moderado a grande, la fuerza bruta rara vez es la estrategia preferida para resolver el problema de la mochila, aunque sirve como base para entender la complejidad del problema y motivar la búsqueda de algoritmos más eficientes.

### Generalización de la estrategia de fuerza bruta

La estrategia de fuerza bruta se puede aplicar a una amplia variedad de problemas computacionales, no solo al problema de la mochila.
La idea central es explorar todas las posibles soluciones candidatas y seleccionar la mejor o una que cumpla con ciertos criterios.
En esencia, a este tipo de algoritmos se les conoce como **algoritmos de búsqueda exhaustiva**.
La fuerza bruta es una estrategia de diseño de algoritmos que se basa en la idea de que, si se pueden enumerar todas las posibles soluciones a un problema, entonces se puede encontrar la solución óptima o una solución válida mediante la exploración sistemática de todas ellas.

**Cómo generar un algoritmo de fuerza bruta**:

1. **Definir el espacio de búsqueda**: Identifica una manera de representar cada posible solución al problema. Esto puede ser una lista, un conjunto, una cadena, etc.
2. **Generar soluciones candidatas**: Implementa un generador o función que produzca todas las posibles soluciones candidatas. Esto puede implicar el uso de recursión, iteración o combinaciones.
3. **Evaluar cada solución**: Para cada solución candidata generada, evalúa si es válida o no. Esto puede implicar verificar restricciones, calcular costos, etc.
4. **Seleccionar la mejor solución**: Mantén un registro de la mejor solución encontrada hasta el momento. Esto puede implicar comparar valores, costos o cualquier otro criterio relevante.
5. **Devolver la mejor solución**: Al final del proceso, devuelve la mejor solución encontrada o una lista de soluciones válidas.

```python
def fuerza_bruta(instancia):
    # Generar todas las soluciones candidatas
    for solucion_candidata in generar_soluciones_candidatas(instancia):
        if not es_valida(solucion_candidata, instancia):  # ¿No es válida?
            continue  # Pasar a la siguiente solución candidata
        valor = evaluar(solucion_candidata, instancia)
        
        if valor > mejor_valor:  # ¿Es mejor que la mejor solución encontrada?
            # Actualizar la mejor solución
            mejor_valor, mejor_solucion = valor, solucion_candidata

    return mejor_valor, mejor_solucion
```

### Ejercicios adicionales

4. **Análisis técnico**:
   - Para un conjunto de $n$ elementos, calcula el número total de subconjuntos generados por el algoritmo de fuerza bruta.
   - Discute con un asistente de IA cómo este número crece exponencialmente y cómo afecta la viabilidad del algoritmo para valores grandes de $n$.

5. **Explorar alternativas**:
   - Investiga otras estrategias de diseño de algoritmos, como vuelta atrás o algoritmos voraces, que puedan aplicarse al problema de la mochila.
   - Implementa una solución utilizando una de estas estrategias y compárala con la solución de fuerza bruta.

6. **Visualización del espacio de búsqueda**:
   - Diseña un programa que visualice el espacio de búsqueda del problema de la mochila, mostrando todos los subconjuntos generados y destacando los válidos.
   - Reflexiona sobre cómo esta visualización puede ayudar a entender la complejidad del problema.

## 9.4 Vuelta atrás (Backtracking)

La estrategia de **vuelta atrás** (o *backtracking* en inglés) es una técnica algorítmica que busca soluciones a un problema construyendo la solución incrementalmente, un paso a la vez. Si en algún momento la construcción parcial de la solución no puede completarse para formar una solución válida, el algoritmo "vuelve atrás" a un estado anterior y explora un camino diferente. Esta técnica es particularmente útil para resolver problemas de satisfacción de restricciones y problemas de optimización donde se busca una solución entre un gran número de posibilidades.

A diferencia de la fuerza bruta, que explora ciegamente todo el espacio de búsqueda, el backtracking poda el espacio de búsqueda al descartar de antemano las ramas que no pueden conducir a una solución válida. Esto se logra definiendo funciones de "poda" o "validación" que determinan si una solución parcial es prometedora.

### El Problema de las 8 Reinas

Un ejemplo clásico para ilustrar la estrategia de vuelta atrás es el **Problema de las 8 Reinas**. Como se mencionó en la Sección 9.1, el problema consiste en colocar ocho reinas en un tablero de ajedrez de $8 \times 8$ de tal manera que ninguna reina ataque a otra. Esto significa que no puede haber dos reinas en la misma fila, la misma columna o la misma diagonal.

La fuerza bruta para este problema implicaría generar todas las posibles combinaciones de posiciones para las 8 reinas en el tablero (lo que sería un número inmenso) y luego verificar si cada combinación es válida. Sin embargo, con el backtracking, podemos ser mucho más eficientes.
En este caso, el objetivo es colocar 8 reinas en un tablero de ajedrez de $8 \times 8$ de manera que ninguna reina ataque a otra. Esto significa que no puede haber dos reinas en la misma fila, columna o diagonal. Generalizando, para un tablero de $n \times n$, buscamos una configuración válida de $n$ reinas.

### Representación del problema

El tablero de $n \times n$ puede representarse mediante un vector $p$ de tamaño $n$, donde $p[j] = i$ indica que hay una reina en la columna $j$ y la fila $i$. Por ejemplo, si $p = [0, 2, 4, 1]$, esto significa que:

- En la columna $0$, la reina está en la fila $0$.
- En la columna $1$, la reina está en la fila $2$.
- En la columna $2$, la reina está en la fila $4$.
- En la columna $3$, la reina está en la fila $1$.

Para implementar una solución al problema de las 8 reinas utilizando la estrategia de vuelta atrás, seguimos los siguientes pasos:

1. **Definir el espacio de búsqueda**: Representamos el tablero como un vector $p$ de tamaño $n$, donde $p[j] = i$ indica que hay una reina en la columna $j$ y la fila $i$.
2. **Construir soluciones parciales**: Colocamos una reina en una columna a la vez, asegurándonos de que no ataque a ninguna de las reinas ya colocadas.
3. **Validar soluciones parciales**: Verificamos que la posición de la reina en la columna actual no esté en conflicto con las reinas ya colocadas. Esto implica comprobar que no estén en la misma fila ni en la misma diagonal.
4. **Retroceder si es necesario**: Si no es posible colocar una reina en la columna actual, retrocedemos a la columna anterior y probamos una posición diferente.
5. **Completar la solución**: Si logramos colocar reinas en todas las columnas, hemos encontrado una solución válida.

A continuación, se presenta un ejemplo de implementación en Python:

```python
def es_valida(tablero, fila, columna):
   for col_anterior in range(columna):
      fila_anterior = tablero[col_anterior]
      # Verificar si están en la misma fila o en la misma diagonal
      if fila_anterior == fila or abs(fila_anterior - fila) == abs(col_anterior - columna):
         return False
   return True

def resolver_reinas(tablero, columna, n):
   if columna == n:
      # Se ha encontrado una solución válida
      print(tablero)
      return

   for fila in range(n):
      if es_valida(tablero, fila, columna):
         tablero[columna] = fila
         resolver_reinas(tablero, columna + 1, n)
         # No es necesario "deshacer" explícitamente, ya que sobrescribimos en la siguiente iteración

def problema_reinas(n):
   tablero = [-1] * n  # Inicializar el tablero con valores inválidos
   resolver_reinas(tablero, 0, n)

# Ejemplo de uso:
problema_reinas(8)
```

### Análisis de Complejidad para `resolver_reinas`

El algoritmo de vuelta atrás para el problema de las $n$ reinas tiene una complejidad temporal en el peor caso de $O(n!)$, ya que en el peor de los casos explora todas las permutaciones posibles de las reinas en el tablero. Sin embargo, en la práctica, la poda del espacio de búsqueda reduce significativamente el número de configuraciones exploradas.

Este enfoque es mucho más eficiente que la fuerza bruta y es aplicable a una amplia variedad de problemas de satisfacción de restricciones, como el Sudoku, el problema del caballero y muchos otros acertijos de lógica.
En la práctica, el backtracking se usa para resolver problemas de optimización y búsqueda como son:

- Problemas de asignación: asignar tareas a trabajadores, asignar recursos a proyectos, etc.
- Problemas de programación entera: encontrar la mejor combinación de variables enteras que satisfacen ciertas restricciones.
- Problemas de secuenciación: encontrar la mejor secuencia de tareas o eventos que minimice el tiempo total o maximice la eficiencia.
- Problemas de planificación: asignar tareas a recursos en un horario determinado.

### Ejercicios

1. **Explorar variantes del problema de las $n$ reinas**:
   - Investiga y describe variantes del problema de las $n$ reinas, como el problema de las $n$ reinas en un tablero toroidal o el problema de las $n$ reinas con restricciones adicionales (por ejemplo, algunas casillas bloqueadas).
   - Implementa una solución para una de estas variantes utilizando vuelta atrás.
   - Discute con un asistente de IA cómo estas variantes afectan la complejidad del problema.

2. **Comparar estrategias**:
   - Implementa el problema de las $n$ reinas utilizando fuerza bruta y vuelta atrás.
   - Analiza y compara el tiempo de ejecución de ambos enfoques para diferentes valores de $n$.
   - Reflexiona sobre los casos en los que la vuelta atrás es más eficiente.

3. **Buscar aplicaciones reales**:
   - Identifica un problema cotidiano que pueda modelarse como un problema de vuelta atrás (por ejemplo, planificación de horarios o resolución de rompecabezas).
   - Define los datos de entrada, los datos de salida y las restricciones.
   - Discute con un asistente de IA cómo optimizar la solución para este problema.

4. **Visualización del espacio de búsqueda**:
   - Diseña un programa que visualice el espacio de búsqueda del problema de las $n$ reinas, mostrando las configuraciones parciales y las podas realizadas.
   - Reflexiona sobre cómo esta visualización puede ayudar a entender la eficiencia de la estrategia de vuelta atrás.

5. **Explorar problemas similares**:
   - Investiga otros problemas que puedan resolverse con vuelta atrás, como el Sudoku, el problema del caballero o el problema de partición de conjuntos.
   - Implementa una solución para uno de estos problemas y analiza su complejidad.

6. **Explorar alternativas**:
   - Investiga otras estrategias de diseño de algoritmos, como programación dinámica o algoritmos voraces, que puedan aplicarse a problemas similares al de las $n$ reinas.
   - Implementa una solución utilizando una de estas estrategias y compárala con la solución de vuelta atrás.

## 9.5 Algoritmos Voraces

Dentro de las estrategias para diseñar algoritmos, los **algoritmos voraces** destacan por su simplicidad y eficiencia en muchos casos. La idea central es tomar la mejor decisión posible en cada paso local, con la esperanza de que esta secuencia de óptimos locales conduzca a un óptimo global.

Esta toma de decisiones inmediatas suele basarse en una **heurística**, que es una regla práctica o un atajo mental. Una heurística no garantiza la solución perfecta, pero busca una solución "suficientemente buena" de manera rápida. En los algoritmos voraces, la heurística es precisamente la elección localmente óptima en cada etapa del algoritmo. El desafío teórico reside en demostrar si esta heurística voraz produce efectivamente la solución óptima para un problema dado.

### El Problema del Árbol de Expansión Mínima

Para ilustrar un algoritmo voraz, consideremos el **Problema del Árbol de Expansión Mínima** (*Minimum Spanning Tree* o *MST* por sus siglas en inglés): Supongamos que la UAEM ha decidido construir un nuevo campus, y se desea instalar la red eléctrica de manera que todos los edificios estén interconectados y el costo total de la instalación sea el menor posible.
Podemos representar a cada edificio con un punto, y las conexiones entre ellos como líneas que los unen; además, agregamos un costo a cada conexión con un número real positivo que representa el costo de cableado entre los edificios.
A esta representación se le conoce como **grafo ponderado**.

Formalmente, tenemos un **grafo no dirigido y conectado** $G = (V, E)$, donde $V$ es un conjunto finito de **vértices** (nodos) y $E\subseteq \binom{V}{2} = \left\lbrace \{u, v\} \mid u, v \in V, u \neq v \right\rbrace$
 es un conjunto finito de **aristas** (conexiones entre pares de vértices). Cada arista $e \in E$ tiene asociado un **peso** $w(e) \in \mathbf{R}$, que representa un "costo" o "distancia". Un grafo es conectado si se puede ir de cualquier vértice a cualquier otro mediante un camino a través de las aristas.

Un **árbol de expansión** de $G$ es un subgrafo $T = (V, E_T)$ tal que $E_T \subseteq E$, $T$ incluye a todos los vértices de $G$, y $T$ es un árbol (no contiene ciclos y es conectado). Para un grafo con $|V|$ vértices, un árbol de expansión siempre tendrá $|V|-1$ aristas.

El **Árbol de Expansión Mínima (MST)**, $T^*$, es un árbol de expansión de $G$ cuya suma total de pesos es la menor posible. Es decir, buscamos un subconjunto de aristas $E_{T^*} \subseteq E$ tal que $(V, E_{T^*})$ sea un árbol de expansión y $\sum_{e \in E_{T^*}} w(e)$ sea mínima.

### Algoritmo de Prim

El Algoritmo de Prim es un método voraz para encontrar un MST. Comienza con un único edificio (vértice) y, paso a paso, añade el cable (arista) más barato que conecta un edificio ya cableado con uno que aún no lo está, hasta que todos los edificios están interconectados.

Los pasos son los siguientes:

1. Comenzar con un conjunto de vértices en el MST en construcción, $V_{MST}$, que inicialmente contiene solo un vértice arbitrario $v_0 \in V$. El conjunto de aristas del MST en construcción, $E_{MST}$, está vacío.
2. Mientras el número de vértices en $V_{MST}$ sea menor que el total de vértices en $V$:
    1. Buscar entre todas las aristas $e = (u, v) \in E$ aquella de menor peso $w(e)$ tal que uno de sus extremos ($u$) esté en $V_{MST}$ y el otro extremo ($v$) no esté en $V_{MST}$ (es decir, $v \in V \setminus V_{MST}$).
    2. Añadir el vértice $v$ (el extremo que no estaba en $V_{MST}$) al conjunto $V_{MST}$.
    3. Añadir la arista $e = (u, v)$ al conjunto $E_{MST}$.

La decisión voraz en cada paso 2.1 es elegir la arista de menor peso que une un vértice "dentro" del MST parcial con uno "fuera". Esta elección localmente óptima construye progresivamente el MST global.

### Representación de Grafos en Python

Una forma conveniente de representar un grafo ponderado en Python es usando un **diccionario de diccionarios**. El diccionario exterior tiene como claves los vértices del grafo. Cada clave mapea a otro diccionario que representa los vecinos de ese vértice. En el diccionario interior, las claves son los vértices vecinos, y los valores son los pesos de las aristas correspondientes.

Por ejemplo, un grafo con vértices A, B, C y D, con aristas (A, B) de peso 2, (A, C) de peso 3, (B, C) de peso 1 y (C, D) de peso 4, se representaría así:

```python
grafo_ejemplo_representacion = {
    'A': {'B': 2, 'C': 3},
    'B': {'A': 2, 'C': 1},
    'C': {'A': 3, 'B': 1, 'D': 4},
    'D': {'C': 4}
}
```

Observa que como es un grafo no dirigido, si existe la arista (A, B) con peso 2, también debe existir la arista (B, A) con el mismo peso.

### Montículos (Heaps)

Dentro de las estrategias para diseñar algoritmos, es crucial entender cómo ciertas **estructuras de datos** nos permiten manipular colecciones de datos de manera eficiente. Una estructura de datos es una forma particular de organizar y almacenar datos en una computadora para que puedan ser utilizados de manera eficiente. Las estructuras de datos son la base de muchos algoritmos eficientes.

Un **conjunto dinámico** es una colección de elementos cuyo tamaño puede cambiar; soporta operaciones como añadir, eliminar, buscar, y encontrar el mínimo o el máximo de sus elementos.

Un **montículo** (heap) es una estructura de datos que implementa eficientemente algunas operaciones de **conjuntos dinámicos**, particularmente encontrar y extraer el elemento mínimo o máximo. Aunque conceptualmente es un **árbol binario** (un grafo conectado sin ciclos con una estructura jerárquica donde cada nodo tiene como máximo dos hijos), en la práctica, un montículo binario completo o casi completo se puede representar de manera muy eficiente usando una simple lista o arreglo. El módulo `heapq` de Python utiliza esta representación basada en listas.

La clave de la eficiencia del montículo es la **propiedad del montículo**. Para un **montículo mínimo (min-heap)**, que es el que implementa `heapq`, esta propiedad dice que para cualquier nodo que no sea la raíz, el valor del nodo es mayor o igual que el valor de su nodo padre. Esto implica que el elemento más pequeño siempre se encuentra en la raíz del montículo (en el primer índice de la lista).

La belleza de la representación en lista es que las relaciones padre-hijo se calculan fácilmente: si un nodo está en el índice $i$ de la lista (asumiendo que el índice 0 es la raíz):

- Su nodo padre está en el índice $\lfloor (i-1) / 2 \rfloor$ (para $i > 0$).
- Sus nodos hijos están en los índices $2i + 1$ (hijo izquierdo) y $2i + 2$ (hijo derecho), si existen.

Veamos cómo las operaciones principales de `heapq` (`heappush`, `heappop`, `heapify`) manipulan la lista para mantener la propiedad del montículo. Usaremos los términos en español "monticulo", "flotar" (subir) y "hundir" (bajar) para describir la lógica interna.

#### `agregar_a_monticulo(monticulo, elemento)`: Insertar un Elemento

Cuando se inserta un nuevo elemento en un montículo mínimo, se coloca inicialmente al final de la lista (para mantener la estructura de árbol casi completa). Luego, para restaurar la propiedad del montículo, el elemento se compara con su padre. Si es menor que su padre, se intercambian. Este proceso de comparación e intercambio ("flotar" o "bubble up") continúa hasta que el elemento es mayor o igual que su padre, o hasta que llega a la raíz.

Aquí hay una función simplificada que ilustra la lógica de "flotar":

```python
def flotar(heap, pos):
   # Mantiene la propiedad del montículo subiendo el elemento en 'pos'
   padre = (pos - 1) // 2  # Calcula la posición del padre

   # Mientras no sea la raíz (pos > 0) y el elemento actual sea menor que su padre
   while pos > 0 and heap[pos] < heap[padre]:
      # Intercambiar el elemento actual con su padre
      heap[pos], heap[padre] = heap[padre], heap[pos]
      pos = padre  # Moverse a la nueva posición (la del padre)
      padre = (pos - 1) // 2  # Recalcular la posición del nuevo padre

# Una implementación conceptual simplificada de agregar_a_monticulo
def agregar(heap, elem):
   heap.append(elem)  # Añadir el nuevo elemento al final de la lista
   flotar(heap, len(heap) - 1)  # Restaurar la propiedad del montículo subiendo el nuevo elemento

# Ejemplo de uso conceptual (usando la función interna simplificada)
mi_heap = []
agregar(mi_heap, 4)
print("Después de agregar(4):", mi_heap)  # [4]
agregar(mi_heap, 1)
print("Después de agregar(1):", mi_heap)  # [1, 4]
agregar(mi_heap, 7)
print("Después de agregar(7):", mi_heap)  # [1, 4, 7]
agregar(mi_heap, 3)
print("Después de agregar(3):", mi_heap)  # [1, 3, 7, 4]
```

#### `extraer_minimo(monticulo)`: Extraer el Elemento Mínimo

La operación `extraer_minimo` remueve y retorna el elemento más pequeño (la raíz). Para llenar el hueco en la raíz y mantener la estructura de montículo casi completa, el último elemento de la lista se mueve a la raíz. Luego, para restaurar la propiedad del montículo, este nuevo elemento en la raíz se "hunde" (bubble down o down-heap). Esto implica compararlo con sus hijos y, si es mayor que el menor de sus hijos, se intercambian. Este proceso de comparación e intercambio con el menor de los hijos continúa hasta que el elemento es menor o igual que ambos hijos, o hasta que llega a una hoja.

Aquí hay una función simplificada que ilustra la lógica de "hundir":

```python
def hundir(heap, pos, tam):
   # Mantiene la propiedad del montículo bajando el elemento en 'pos'
   # 'tam' es el número actual de elementos válidos en el montículo

   mientras True:
      izq = 2 * pos + 1  # Posición del hijo izquierdo
      der = 2 * pos + 2  # Posición del hijo derecho
      menor = pos        # Inicialmente asumimos que el nodo actual es el menor

      # Determinar la posición del hijo más pequeño (si existe)
      if izq < tam and heap[izq] < heap[menor]:
         menor = izq
      if der < tam and heap[der] < heap[menor]:
         menor = der

      # Si el menor hijo no es el nodo actual, necesitamos intercambiar
      if menor != pos:
         # Intercambiar el elemento actual con el menor de sus hijos
         heap[pos], heap[menor] = heap[menor], heap[pos]
         pos = menor  # Continuar hundiendo desde la nueva posición
      else:
         # El elemento actual ya es menor o igual que sus hijos, la propiedad se cumple
         break

# Una implementación conceptual simplificada de extraer_minimo
def extraer_minimo(heap):
   if not heap:
      return None  # Montículo vacío

   # El elemento más pequeño es la raíz
   raiz = heap[0]

   # Mover el último elemento a la raíz
   heap[0] = heap[-1]
   heap.pop()  # Remover el último elemento original

   # Restaurar la propiedad del montículo hundiendo el nuevo elemento en la raíz
   if heap:  # Asegurarse de que el montículo no esté vacío después del pop
      hundir(heap, 0, len(heap))

   return raiz

# Ejemplo de uso conceptual (usando las funciones internas simplificadas)
mi_heap_ejemplo_pop = [1, 3, 2, 7, 4, 5, 6]  # Asumimos que ya es un min-heap

print("Montículo antes de extraer_minimo:", mi_heap_ejemplo_pop)
minimo_extraido = extraer_minimo(mi_heap_ejemplo_pop)
print("Elemento extraído:", minimo_extraido)  # 1
print("Montículo después de extraer_minimo:", mi_heap_ejemplo_pop)  # [2, 3, 5, 7, 4, 6]
```

#### `convertir_en_monticulo(lista)`: Construir un Montículo desde una Lista

La operación `convertir_en_monticulo` toma una lista cualquiera y la reorganiza para que cumpla la propiedad del montículo in-place. La forma eficiente de hacerlo es aplicando la operación de "hundir" (usando la función `hundir`) desde los últimos nodos que no son hojas (los padres de las hojas) hacia la raíz. Cualquier nodo hoja ya cumple trivialmente la propiedad del montículo.

La posición del último nodo no hoja en una lista de tamaño $n$ es $\lfloor (n/2) - 1 \rfloor$.

Aquí hay una función simplificada que ilustra la lógica de `convertir_en_monticulo`:

```python
def convertir_en_monticulo(lista):
    """
    Convierte una lista en un montículo in-place aplicando 'hundir' desde los nodos no hoja.
    """
    n = len(lista)
    # Empezar desde el último nodo no hoja y trabajar hacia la raíz
    # La posición del último nodo no hoja es n // 2 - 1
    for i in range(n // 2 - 1, -1, -1):
        hundir(lista, i, n) # Aplicar la operación de hundir a este nodo

# Ejemplo de uso conceptual (usando la función interna simplificada)
lista_desordenada_ejemplo_heapify = [7, 1, 3, 4, 5, 2, 6]

print("Lista antes de convertir_en_monticulo:", lista_desordenada_ejemplo_heapify)
convertir_en_monticulo(lista_desordenada_ejemplo_heapify)
print("Lista después de convertir_en_monticulo:", lista_desordenada_ejemplo_heapify) # Ahora es un montículo válido
print("El elemento más pequeño (raíz):", lista_desordenada_ejemplo_heapify[0]) # 1

# Podemos verificar que ahora se comporta como un montículo extrayendo elementos
print("Extrayendo elementos en orden con extraer_minimo:")
while lista_desordenada_ejemplo_heapify:
    print(extraer_minimo(lista_desordenada_ejemplo_heapify), end=" ") # 1 2 3 4 5 6 7
print()
```

Estas implementaciones simplificadas muestran las ideas principales detrás de las operaciones de montículo: cómo se mueven los elementos hacia arriba (`flotar`) o hacia abajo (`hundir`) del árbol (representado en la lista) mediante comparaciones e intercambios para mantener la crucial propiedad del montículo. La función `convertir_en_monticulo` utiliza `hundir` para construir eficientemente un montículo a partir de una lista existente. La implementación real en el módulo `heapq` de Python es más optimizada, pero la lógica fundamental es la misma.

La eficiencia de estas operaciones (típicamente $O(\log n)$ para `agregar_a_monticulo` y `extraer_minimo`, y $O(n)$ para `convertir_en_monticulo`, donde $n$ es el número de elementos) es lo que hace que los montículos sean tan valiosos en algoritmos donde se necesita acceder repetidamente al elemento mínimo (o máximo) de un conjunto dinámico, como en el Algoritmo de Prim.

### Implementación del Algoritmo de Prim en Python

Aquí tienes la implementación del Algoritmo de Prim usando la representación de diccionario de diccionarios y variables en español. Se emplea el módulo `heapq` para manejar eficientemente la selección de la arista de menor peso.

```python
import heapq

def algoritmo_prim(grafo):
    if not grafo:
        return [], 0 # Manejar caso de grafo vacío

    vertices = list(grafo.keys())
    # Seleccionar un vértice inicial arbitrario para empezar a construir el MST
    vertice_inicio = vertices[0]

    # Conjunto para rastrear los vértices ya incluidos en el MST
    vertices_en_mst = {vertice_inicio}

    # Lista para almacenar las aristas que formarán el MST resultante
    aristas_mst = []

    # Cola de prioridad (min-heap) para mantener las aristas candidatas:
    # (peso, origen, destino)
    # Inicializamos con todas las aristas saliendo del vértice inicial
    candidatos_aristas = []
    for vecino, peso in grafo[vertice_inicio].items():
        heapq.heappush(candidatos_aristas, (peso, vertice_inicio, vecino))

    # Continuar hasta que todos los vértices estén en el MST
    mientras len(vertices_en_mst) < len(vertices):
        # Extraer la arista con el menor peso de la cola de prioridad
        peso_arista, origen, destino = heapq.heappop(candidatos_aristas)

        # Si el vértice destino ya está en el MST, esta arista crearía un ciclo; la ignoramos.
        if destino in vertices_en_mst:
            continue

        # Añadir el nuevo vértice y la arista al MST
        vertices_en_mst.add(destino)
        aristas_mst.append((origen, destino))

        # Añadir las nuevas aristas que salen del vértice recién añadido ('destino')
        # a la cola de prioridad, si conectan a un vértice que aún no está en el MST.
        for vecino_de_destino, peso_a_vecino in grafo[destino].items():
            if vecino_de_destino not in vertices_en_mst:
                heapq.heappush(candidatos_aristas, (peso_a_vecino, destino, vecino_de_destino))

    # Calcular el peso total sumando los pesos de las aristas seleccionadas
    peso_total_mst = sum(grafo[u][v] for u, v in aristas_mst)

    return aristas_mst, peso_total_mst

# --- Ejemplo de uso ---

# Ejemplo de grafo representando edificios (letras) y costos de conexión (enteros)
grafo_uaem_simplificado = {
    'Rectoria': {'Fac_Ciencias': 5, 'Deportes': 8, 'Ingenieria': 12},
    'Fac_Ciencias': {'Rectoria': 5, 'Deportes': 9, 'Posgrado_A': 4},
    'Deportes': {'Rectoria': 8, 'Fac_Ciencias': 9, 'Posgrado_A': 7, 'Biblioteca': 6},
    'Ingenieria': {'Rectoria': 12, 'Biblioteca': 3},
    'Posgrado_A': {'Fac_Ciencias': 4, 'Deportes': 7, 'Biblioteca': 2},
    'Biblioteca': {'Deportes': 6, 'Ingenieria': 3, 'Posgrado_A': 2}
}

# Ejecutar el algoritmo
aristas_minimas, costo_total = algoritmo_prim(grafo_uaem_simplificado)

print("Aristas seleccionadas para la red eléctrica mínima (MST):", aristas_minimas)
print("Costo total mínimo del cableado (Peso del MST):", costo_total)

```

La complejidad temporal de esta implementación del Algoritmo de Prim utilizando una cola de prioridad binaria (como la de `heapq`) sobre un grafo representado con lista de adyacencia (el diccionario de diccionarios lo simula bien) es $O(|E| \log |V|)$, donde $|E|$ es el número de aristas y $|V|$ es el número de vértices. Esto se debe a que cada arista se inserta en la cola de prioridad a lo sumo una vez, y cada extracción de la arista mínima toma tiempo logarítmico con respecto al número de elementos en la cola.

### Ejercicios

1. **Explorar variantes del problema del Árbol de Expansión Mínima**:
   - Investiga variantes del problema del Árbol de Expansión Mínima, como el problema del Árbol de Expansión Máxima o el Árbol de Expansión Mínima con restricciones (por ejemplo, excluir ciertas aristas).
   - Implementa una solución para una de estas variantes utilizando un enfoque voraz.
   - Discute con un asistente de IA cómo estas variantes afectan la aplicabilidad del algoritmo de Prim.

2. **Comparar estrategias**:
   - Implementa el problema del Árbol de Expansión Mínima utilizando el algoritmo de Prim y el algoritmo de Kruskal (puedes preguntar a un asistente de IA sobre el algoritmo de Kruskal y su implementación).
   - Analiza y compara el tiempo de ejecución de ambos enfoques para diferentes grafos (por ejemplo, grafos densos y grafos dispersos).
   - Reflexiona sobre los casos en los que cada estrategia es más eficiente.

3. **Buscar aplicaciones reales**:
   - Identifica un problema cotidiano que pueda modelarse como un problema de optimización voraz (por ejemplo, planificación de rutas o asignación de recursos).
   - Define los datos de entrada, los datos de salida y las restricciones.
   - Discute con un asistente de IA cómo optimizar la solución para este problema utilizando un enfoque voraz.

4. **Visualización del proceso voraz**:
   - Diseña un programa que visualice el proceso del algoritmo de Prim, mostrando cómo se seleccionan las aristas en cada paso.
   - Reflexiona sobre cómo esta visualización puede ayudar a entender la lógica detrás de los algoritmos voraces.

5. **Explorar problemas similares**:
   - Investiga otros problemas que puedan resolverse con algoritmos voraces, como el problema del cambio de monedas, el problema de Huffman o el problema del viajante de comercio (usando una heurística voraz).
   - Implementa una solución para uno de estos problemas y analiza su eficiencia.

6. **Explorar limitaciones de los algoritmos voraces**:
   - Investiga problemas donde los algoritmos voraces no garantizan una solución óptima (por ejemplo, el problema del viajante de comercio o el problema de la mochila 0-1).
   - Implementa un ejemplo donde un algoritmo voraz falle en encontrar la solución óptima.
   - Discute con un asistente de IA cómo identificar problemas donde los algoritmos voraces no son adecuados.

Okay, revisaré la sección 9.6 para incorporar tus comentarios: usar "K" para la tabla, mostrar la reconstrucción de la solución con una tabla de decisiones, y evitar el término "bottom-up".

## 9.6 Programación Dinámica

Hemos explorado algoritmos voraces que toman decisiones localmente óptimas. Ahora, nos adentraremos en la **Programación Dinámica**, una técnica poderosa para resolver problemas que, a primera vista, podrían parecer intratables debido a la explosión de posibilidades, pero que en realidad tienen una estructura subyacente que podemos explotar.

La Programación Dinámica se aplica a problemas que exhiben dos propiedades clave:

1. **Subproblemas Superpuestos:** El problema se puede descomponer en subproblemas más pequeños, y los mismos subproblemas se resuelven repetidamente al abordar el problema original.
2. **Estructura Óptima de Subestructura:** Una solución óptima al problema original se puede construir a partir de soluciones óptimas a sus subproblemas.

Si un problema tiene estas propiedades, podemos usar la Programación Dinámica para evitar recalcular las soluciones a los subproblemas superpuestos.

### Memoización: El Arte de Recordar

Una técnica fundamental en Programación Dinámica es la **memoización**. La palabra "memoización" proviene del latín "memorandum", que significa "cosa que se debe recordar". En el contexto de algoritmos, la memoización es una técnica de optimización utilizada para acelerar programas de computadora al almacenar los resultados de llamadas a funciones costosas y devolver el resultado almacenado cuando las mismas entradas ocurren nuevamente.

Consideremos el cálculo del $n$-ésimo número de Fibonacci, donde $F_0 = 0$, $F_1 = 1$, y $F_n = F_{n-1} + F_{n-2}$ para $n > 1$. Una implementación recursiva directa es simple:

```python
def fibonacci_recursivo(n):
    if n <= 1:
        return n
    else:
        return fibonacci_recursivo(n - 1) + fibonacci_recursivo(n - 2)

# Ejemplo:
# print(f"Fibonacci(6) = {fibonacci_recursivo(6)}") # Descomentar para ver la lentitud en números mayores
```

Si intentamos calcular `fibonacci_recursivo(6)`, veremos que se calculan `fibonacci_recursivo(5)` y `fibonacci_recursivo(4)`. Para calcular `fibonacci_recursivo(5)`, se calculan `fibonacci_recursivo(4)` y `fibonacci_recursivo(3)`. Notamos que `fibonacci_recursivo(4)` se calcula múltiples veces. A medida que $n$ crece, la cantidad de cálculos repetidos aumenta exponencialmente, haciendo que esta implementación sea muy ineficiente ($O(2^n)$).

Aquí es donde entra la memoización. Podemos usar un diccionario para almacenar los resultados de las llamadas a la función a medida que se calculan. Antes de calcular `fibonacci_recursivo(n)`, verificamos si el resultado para $n$ ya está en nuestro diccionario. Si es así, lo devolvemos directamente. Si no, lo calculamos, lo almacenamos en el diccionario y luego lo devolvemos.

```python
def fibonacci_memoizado(n, cache={}):
    # Verificamos si el resultado ya está en el cache (diccionario)
    if n in cache:
        return cache[n]

    # Casos base
    if n <= 1:
        resultado = n
    else:
        # Calculamos el resultado recursivamente y lo almacenamos en el cache
        resultado = fibonacci_memoizado(n - 1, cache) + fibonacci_memoizado(n - 2, cache)

    cache[n] = resultado
    return resultado

# Ejemplo:
print(f"Fibonacci(6) con memoización = {fibonacci_memoizado(6)}")
print(f"Fibonacci(10) con memoización = {fibonacci_memoizado(10)}")
# El diccionario 'cache' ahora guarda los resultados calculados.
```

Con la memoización, cada número de Fibonacci se calcula solo una vez. La complejidad temporal se reduce drásticamente a $O(n)$, ya que solo visitamos cada valor de $n$ hasta el deseado una vez.

Python incluso proporciona una forma sencilla de aplicar memoización usando el decorador `@functools.cache` (o `@functools.lru_cache` para versiones anteriores de Python o si se necesita un límite de tamaño para el cache):

```python
import functools

@functools.cache
def fibonacci_cache(n):
    if n <= 1:
        return n
    else:
        return fibonacci_cache(n - 1) + fibonacci_cache(n - 2)

# Ejemplo:
print(f"Fibonacci(6) con functools.cache = {fibonacci_cache(6)}")
print(f"Fibonacci(10) con functools.cache = {fibonacci_cache(10)}")
# El decorador @functools.cache maneja automáticamente el almacenamiento y la recuperación del cache.
```

El decorador `@functools.cache` hace exactamente lo que hicimos manualmente con el diccionario, pero de forma más limpia y eficiente.

### Programación Dinámica Aplicada: El Problema de la Mochila (0/1 Knapsack)

Recordemos el Problema de la Mochila (0/1 Knapsack) de la Sección 9.3.1: Dado un conjunto de $n$ ítems, cada uno con un peso $w_i$ y un valor $v_i$, y una mochila con capacidad máxima $W$, seleccionar un subconjunto de ítems para maximizar el valor total sin exceder el peso máximo. A diferencia de la fuerza bruta, que prueba todas las combinaciones, y donde una solución voraz podría no ser óptima (tomar el ítem de mayor valor por peso podría no dejar espacio para ítems más valiosos en conjunto), la programación dinámica proporciona una solución óptima.

Este problema tiene **subproblemas superpuestos** y **estructura óptima de subestructura**. Un subproblema podría ser: ¿Cuál es el valor máximo que podemos obtener con los primeros $i$ ítems y una capacidad de mochila de $j$? La solución al problema original (con $n$ ítems y capacidad $W$) depende de las soluciones a subproblemas con menos ítems y/o menor capacidad.

Podemos definir una relación de recurrencia para resolver esto. Sea $K(i, j)$ el valor máximo que se puede obtener utilizando solo los primeros $i$ ítems y con una capacidad de mochila de $j$.

Para calcular $K(i, j)$, consideramos el $i$-ésimo ítem:

- **Caso 1: No incluimos el $i$-ésimo ítem en la mochila.** En este caso, el valor máximo es el mismo que el que obtendríamos con los primeros $i-1$ ítems y la misma capacidad $j$.
  $K(i, j) = K(i-1, j)$

- **Caso 2: Incluimos el $i$-ésimo ítem en la mochila.** Esto solo es posible si el peso del $i$-ésimo ítem ($w_i$) no excede la capacidad actual $j$. Si lo incluimos, el valor total será el valor del $i$-ésimo ítem ($v_i$) más el valor máximo que podríamos obtener con los primeros $i-1$ ítems y la capacidad restante ($j - w_i$).
  $K(i, j) = v_i + K(i-1, j - w_i)$ (si $j \geq w_i$)

Combinando ambos casos, tomamos el máximo de las dos opciones:

$$
K(i, j) = \begin{cases}
K(i-1, j) & \text{si } j < w_{i-1} \\
\max(K(i-1, j), v_{i-1} + K(i-1, j - w_{i-1})) & \text{si } j \geq w_{i-1}
\end{cases}
$$

**Nota:** En la fórmula, usamos $w_{i-1}$ y $v_{i-1}$ para referirnos al peso y valor del $i$-ésimo ítem en una indexación base 0, ya que las listas en Python usan indexación base 0.

Los casos base para la recurrencia son cuando no tenemos ítems o la capacidad es cero:

- $K(0, j) = 0$ para toda capacidad $j \geq 0$ (0 ítems, valor 0).
- $K(i, 0) = 0$ para cualquier número de ítems $i \geq 0$ (capacidad 0, valor 0).

En lugar de resolver esto recursivamente (lo que volvería a tener subproblemas superpuestos), podemos usar una tabla (típicamente una matriz 2D) para almacenar los valores de $K(i, j)$ de manera iterativa, llenando la tabla desde los casos base hacia los problemas de mayor tamaño. La tabla $K$ tendrá dimensiones $(n+1) \times (W+1)$, donde las filas representan el número de ítems considerados (de 0 a $n$) y las columnas representan la capacidad de la mochila (de 0 a $W$).

La solución al problema original (el valor máximo) estará en la celda $K(n, W)$ de la tabla.

Para **reconstruir la solución** (saber qué ítems fueron incluidos), podemos usar una segunda tabla paralela, digamos $P$, de las mismas dimensiones. $P(i, j)$ almacenará la decisión tomada para alcanzar el valor óptimo $K(i, j)$. Por ejemplo, podemos usar 1 si el ítem $i$ fue incluido y 0 si fue excluido.

Aquí tienes una implementación en Python de la solución de Programación Dinámica para el problema de la mochila 0/1, incluyendo la reconstrucción de la solución:

```python
def knapsack_dp(W, w, v):
   n = len(w)

   # Tablas para DP
   K = [[0 for _ in range(W + 1)] for _ in range(n + 1)]  # Tabla de valores máximos
   P = [[0 for _ in range(W + 1)] for _ in range(n + 1)]  # Tabla de decisiones

   # Llenar tablas K y P
   for i in range(n + 1):
      for j in range(W + 1):
         if i > 0 y j > 0:
            wi = w[i - 1]  # Peso del ítem actual
            vi = v[i - 1]  # Valor del ítem actual

            excl = K[i - 1][j]  # Caso: excluir el ítem actual
            incl = -1
            if wi <= j:
               incl = vi + K[i - 1][j - wi]  # Caso: incluir el ítem actual

            if incl > excl:
               K[i][j] = incl  # Guardar el valor máximo al incluir
               P[i][j] = 1  # Marcar que el ítem fue incluido
            else:
               K[i][j] = excl  # Guardar el valor máximo al excluir
               P[i][j] = 0  # Marcar que el ítem no fue incluido

   # Reconstruir solución
   sol = []
   i, j = n, W
   mientras i > 0 y j > 0:
      if P[i][j] == 1:  # Si el ítem fue incluido
         sol.append(i - 1)  # Agregar el índice del ítem a la solución
         j -= w[i - 1]  # Reducir la capacidad restante
      i -= 1  # Pasar al ítem anterior

   sol.reverse()  # Invertir para obtener el orden original
   return K[n][W], sol  # Retornar el valor máximo y los ítems seleccionados

# Ejemplo de uso
W = 10
w = [2, 3, 4, 5]
v = [3, 7, 2, 9]

max_val, sel_items = knapsack_dp(W, w, v)

print(f"Valor máximo: {max_val}")  # Imprimir el valor máximo obtenido
print(f"Ítems seleccionados: {sel_items}")  # Imprimir los índices de los ítems seleccionados
# Opcional: Mostrar pesos y valores de los ítems seleccionados
# print("Pesos seleccionados:", [w[i] for i in sel_items])
# print("Valores seleccionados:", [v[i] for i in sel_items])
```

En esta implementación, la tabla `K` almacena el valor óptimo como antes, y la tabla `P` registra la decisión (incluir o excluir el ítem actual) que condujo a ese valor óptimo. Después de llenar las tablas, reconstruimos la solución rastreando las decisiones desde la celda $K(n, W)$ hacia atrás hasta $K(0, j)$, utilizando la información almacenada en `P`.

La complejidad temporal y espacial siguen siendo $O(n \cdot W)$ debido al llenado de las tablas.

### Principios para Diseñar Algoritmos de Programación Dinámica

Diseñar un algoritmo de programación dinámica generalmente implica los siguientes pasos:

1. **Caracterizar la estructura de una solución óptima:** Identificar cómo una solución óptima para el problema original se relaciona con las soluciones óptimas de sus subproblemas. Debe existir la propiedad de **estructura óptima de subestructura**.
2. **Definir recursivamente el valor de una solución óptima:** Establecer una relación de recurrencia que exprese el valor de la solución óptima para un subproblema en términos de los valores de las soluciones óptimas para subproblemas más pequeños.
3. **Calcular el valor de una solución óptima:** Esto se hace de manera iterativa, resolviendo primero los subproblemas más pequeños y almacenando sus soluciones (generalmente en una tabla). Luego, se utilizan estas soluciones almacenadas para resolver subproblemas de mayor tamaño, avanzando progresivamente hasta obtener la solución del problema original. Este enfoque evita la repetición de cálculos de **subproblemas superpuestos**.
4. **(Opcional) Construir una solución óptima a partir de la información calculada:** Si se necesita conocer los elementos específicos que forman la solución óptima (no solo su valor), se puede almacenar información adicional durante el cálculo (como en la tabla $P$ del ejemplo de la mochila) y luego rastrear las decisiones tomadas para reconstruir la solución.

En resumen, la Programación Dinámica es una técnica poderosa que, al identificar y resolver eficientemente subproblemas superpuestos y explotar la estructura óptima de subestructura, puede transformar algoritmos exponenciales en algoritmos con complejidad polinomial (o pseudo-polinomial) para una amplia gama de problemas de optimización y conteo. La memoización (enfoque "de arriba hacia abajo" o top-down con almacenamiento) y el enfoque iterativo (desde los casos base hacia arriba o bottom-up) son las dos estrategias principales para implementar la Programación Dinámica y evitar la redundancia de cálculos.

### Ejercicios

1. **Buscar aplicaciones reales**:
   - Identifica un problema cotidiano que pueda modelarse como un problema de programación dinámica (por ejemplo, planificación de proyectos o asignación de recursos).
   - Define los datos de entrada, los datos de salida y las restricciones.
   - Discute con un asistente de IA cómo optimizar la solución para este problema utilizando programación dinámica.

2. **Visualización de la tabla de decisiones**:
   - Diseña un programa que visualice la tabla de decisiones para el problema de la mochila, mostrando cómo se llenan las celdas y cómo se reconstruye la solución.
   - Reflexiona sobre cómo esta visualización puede ayudar a entender la lógica detrás de la programación dinámica.

3. **Explorar problemas similares**:
   - Investiga otros problemas que puedan resolverse con programación dinámica, como el problema de la subsecuencia común más larga, el problema de edición de cadenas o el problema de partición de conjuntos.
   - Implementa una solución para uno de estos problemas y analiza su complejidad.

4. **Optimización de espacio**:
   - Investiga cómo optimizar el uso de espacio en algoritmos de programación dinámica, como el uso de una sola fila o columna en lugar de una tabla completa.
   - Implementa una versión optimizada del problema de la mochila utilizando esta técnica.
   - Discute con un asistente de IA cómo estas optimizaciones afectan el rendimiento del algoritmo.
