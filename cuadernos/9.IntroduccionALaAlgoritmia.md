# 9. Introducción a la Algoritmia

- *Autor*: [Dr. Mario Abarca](https://www.knkillname.org/)
- *Objetivos*: Comprender los conceptos fundamentales de problemas computacionales y algoritmos; calcular y analizar la complejidad temporal de algoritmos; conocer estrategias de diseño de algoritmos.

Hasta ahora el curso se ha centrado en escribir algoritmos en Python para resolver problemas.
Sin embargo, no hemos discutido cómo medir formalmente (a priori) los recursos que consume un algoritmo.

## 9.1 Problemas computacionales

**Definición** (De Relación): Una **relación** entre dos conjuntos $A$ y $B$ es un subconjunto $R \subseteq A \times B$.
Decimos que *$a$ está relacionado con $b$* si $(a, b) \in R$.

**Ejemplo**: El juego de *piedra, papel o tijera* es una relación entre los conjuntos $A = \{\mathit{piedra}, \mathit{papel}, \mathit{tijera}\}$ y sí mismo. La relación $\mathit{vence}(a, b)$ indica si el elemento $a$ vence al elemento $b$, y está definida como:
$$
\mathit{VENCE} = \{(\mathit{piedra}, \mathit{tijera}), (\mathit{papel}, \mathit{piedra}), (\mathit{tijera}, \mathit{papel})\}
$$

La notación conjuntista no siempre es la más clara para representar relaciones, por lo que a menudo se utilizan otras notaciones como la notación de predicados o la notación de relaciones que son más intuitivas:

- En la *notación de conjuntos*: $(a, b) \in R$
- En la *notación de relación*: $a\,R\,b$
- En la *notación de predicado*: $R(a, b)$

Continuando con el ejemplo de *piedra, papel o tijera*, la relación $\mathit{VENCE}$ puede representarse de las siguientes maneras:

| Notación de conjuntos | Relación | Predicado |
|-----------|----------|-----------|
| $(\mathit{piedra}, \mathit{tijera}) \in \mathit{VENCE}$ | $\mathit{piedra}\,\mathit{VENCE}\,\mathit{tijera}$ | $\mathit{VENCE}(\mathit{piedra}, \mathit{tijera})$ |
| $(\mathit{papel}, \mathit{piedra}) \in \mathit{VENCE}$ | $\mathit{papel}\,\mathit{VENCE}\,\mathit{piedra}$ | $\mathit{VENCE}(\mathit{papel}, \mathit{piedra})$ |
| $(\mathit{tijera}, \mathit{papel}) \in \mathit{VENCE}$ | $\mathit{tijera}\,\mathit{VENCE}\,\mathit{papel}$ | $\mathit{VENCE}(\mathit{tijera}, \mathit{papel})$ |

Una manera alternativa de representar la relación es como una tabla de verdad, donde cada fila representa una relación entre dos elementos.

| $\mathit{VENCE}$  | $\mathit{piedra}$ | $\mathit{papel}$ | $\mathit{tijera}$ |
|-------------------|-------------------|------------------|-------------------|
| $\mathit{piedra}$ | 0                 | 0                | 1                 |
| $\mathit{papel}$  | 1                 | 0                | 0                 |
| $\mathit{tijera}$ | 0                 | 1                | 0                 |

Hay muchos tipos de problemas, como cuando te peleas con tu suegra, o cuando se te cae el café en la computadora, pero para este curso nos centraremos en los problemas computacionales.
Para ello, presentamos primero un ejemplo de física clásica:

**Ejemplo** (de problema formal): Se desea determinar cuánto tiempo tarda una bola de acero de 1 kg en caer desde una altura de 10 m.
Para resolver este problema, se puede utilizar la ecuación de movimiento uniformemente acelerado:
$$
h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2
$$
donde $h(t)$ es la altura en función del tiempo, $h_0$ es la altura inicial (10 m), $v_0$ es la velocidad inicial (0 m/s), $g$ es la aceleración debida a la gravedad (9.81 m/s²) y $t$ es el tiempo en segundos.
Para resolver la ecuación, se puede despejar $t$ y obtener:
$$
t = \sqrt{\frac{2h_0}{g}} \approx 1.43 \text{ s}
$$

Es importante dar un paso atrás y diseccionar este problema en sus partes fundamentales:

- *Datos del problema*: $h_0 = 10$ m, $v_0 = 0$ m/s, $g = 9.81$ m/s²
- *Datos de la solución*: $t \approx 1.43$ s
- *Relación entre los datos*: $h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2$

Más formalmente la relación entre los datos del problema y los datos de la solución se puede expresar como una *relación* entre los conjuntos:

- *Datos del problema*: $D_p = \{(h_0, v_0, g) \mid h_0, v_0, g \in \mathbf{R}\}$
- *Datos de la solución* $D_s = \{t \mid t \in \mathbf{R}\}$
- *Relación entre los datos* $R \subseteq D_p \times D_s$ definida como:

$$
R = \left\{(\vec d, t) \middle | \vec d = (h_0, v_0, g) \wedge h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2\right\}
$$

En este ejemplo, $(10, 0, 9.81) \in D_p$ y $1.43 \in D_s$ son los datos del problema y de la solución respectivamente, y la relación es $(10, 0, 9.81)\,R\,1.43$.
Podemos generalizar este problema así:

**Problema** (de la caída libre):

- **Dados** $h_0 \in \mathbf{R}$, $v_0 \in \mathbf{R}$, $g \in \mathbf{R}$
- **encontrar** $t \in \mathbf{R}$ **tal que** $h(t) = h_0 + v_0\,t - \frac{1}{2}\,g\,t^2$

Asimismo, la solución se generaliza como:

**Solución** (del problema de la caída libre):

- **Dados**: $h_0 \in \mathbf{R}$, $v_0 \in \mathbf{R}$, $g \in \mathbf{R}$
- **Calcular**: $t = \sqrt{\frac{2h_0}{g}}$
- **Devolver**: $t \in \mathbf{R}$

Generalizando este ejemplo arribamos a la siguiente definición:

**Definición** (Problema computacional): Un **problema computacional** es una relación $R \subseteq E \times S$ donde $E$ es el conjunto de **entradas** y $S$ es el conjunto de **salidas**.
La **solución** de un problema computacional es un algoritmo $A: E \to S$ tal que $A(e) = s$ si y solo si $R(e, s)$.

### Problemas computacionales clásicos

#### Buscar un elemento en una lista

Quizá el problema computacional más clásico es el de buscar un elemento en una lista.

- **Dados**: Una lista $L = [a_0, a_1, \dots, a_{n - 1}]$ y un elemento $x$.
- **Encontrar**: Un índice $i\in \{0,1,\ldots, n-1\}$ **tal que** $L[i] = x$ si es que existe, o $i = -1$ si no existe.

**Ejemplo**:

- Entrada: $L = [3, 5, 7, 9]$, $x = 7$
- Salida: $i = 2$

#### Ordenar una lista

El problema de ordenar una lista es uno de los problemas computacionales más clásicos y se puede formular de la siguiente manera:

- **Dados**: Una lista $L = [a_0, a_1, \dots, a_{n - 1}]$.
- **Encontrar**: Una lista $L' = [a'_0, a'_1, \dots, a'_{n - 1}]$ **tal que** $L'$ es una permutación de $L$ y $L'$ está ordenada.

**Ejemplo**:

- Entrada: $L = [8,5,6,9,0,2,1,3,4,7]$
- Salida: $L' = [0,1,2,3,4,5,6,7,8,9]$

#### El problema de la suma de subconjunto

Este problema es equivalente a devolver el cambio de un billete de $m$ usando monedas de denominaciones $c_0, c_1, \dots, c_{n - 1}$ posiblemente con repetición.

- **Dados**: Un monto $m \in \mathbf{N}$ y una lista de monedas $C = [c_0, c_1, \dots, c_{n - 1}]$.
- **Encontrar**: Una sublista $C' = [c'_0, c'_1, \dots, c'_{k - 1}]$ de $C$ **tal que** $c'_0 + c'_1 + \dots + c'_{k - 1} = m$.

**Ejemplo**:

- Entrada: $m = 7$, $C = [1, 2, 2, 5, 10]$
- Salida: $C' = [2, 5]$

#### El problema de las 8 reinas

Este es un ejemplo de un problema *finito* pero interesante.

- **Dados**: Un tablero de ajedrez de $8 \times 8$, así como 8 reinas de ajedrez.
- **Encontrar**: Una lista de posiciones $P = [(x_0, y_0), (x_1, y_1), \ldots, (x_7, y_7)]$ **tal que** al colocar las reinas en esas posiciones, ninguna reina puede atacar a otra.

### Actividades y ejercicios

1. **Identificar problemas computacionales**:
   - Visita [CodeSignal](https://codesignal.com/) y selecciona algunos problemas de programación.
   - Para cada problema, identifica:
     - Los datos de entrada.
     - Los datos de salida.
     - La relación entre los datos de entrada y salida.

2. **Clasificar problemas**:
   - Pregunta a ChatGPT o el asistente de tu elección sobre problemas de búsqueda, ordenamiento y optimización.
   - Clasifica los problemas seleccionados en categorías como búsqueda, ordenamiento, optimización, etc.
   - Reflexiona sobre las estrategias que podrías usar para resolverlos.

3. **Explorar problemas del mundo real**:
   - Piensa en un problema cotidiano que pueda modelarse como un problema computacional.
   - Define los datos de entrada, los datos de salida y la relación entre ellos.
   - Intenta formular un algoritmo para resolver el problema.

## 9.2 Análisis de algoritmos

En la práctica no sólo es importante resolver un problema computacional, sino que la solución se pueda obtener en un tiempo razonable, o que pueda ser ejecutado en un dispositivo con recursos limitados.
Antes de comprar esa computadora de última generación, primero averigua si no es más barato diseñar un algoritmo más eficiente.
Para ello, es importante analizar la complejidad de un algoritmo, que se refiere a la cantidad de recursos (tiempo y espacio) que consume al ejecutarse.

### Complejidad temporal

El número de pasos que un algoritmo requiere para resolver un problema no sólo depende del algoritmo en sí, sino también de la entrada que se le proporciona.
Por ejemplo, consideremos el algoritmo de búsqueda lineal:

```python
def busqueda_lineal(L, x):
    for i in range(len(L)):
        if L[i] == x:
            return i
    return -1
```

En este caso, el número de pasos que toma el algoritmo depende del tamaño de la lista $L$ y del valor de $x$.

- Si $L$ tiene $n$ elementos, el algoritmo tomará $n$ pasos en el peor de los casos (cuando $x$ no está en la lista o está al final).
- Si $x$ está al principio de la lista, el algoritmo tomará 1 paso.
- Si $x$ está en el medio, tomará aproximadamente $n/2$ pasos.
- Si $x$ no está en la lista, tomará $n$ pasos.

Sea $T(e)$ el número de pasos que toma el algoritmo para una entrada $e$, y sean $E_n = \mathbf R^n$ el conjunto de entradas de tamaño $n$, es decir, el conjunto de todas las listas que tienen $n$ elementos.
Deseamos definir una función $T: \mathbf N \to \mathbf N$ que nos permita calcular el número de pasos que toma el algoritmo para una entrada de tamaño $n$.
Para ello tenemos tres principales enfoques:

- **Peor caso**: Es la cantidad máxima de pasos que toma el algoritmo para una entrada de tamaño $n$.
  $$T(n) = \max_{e \in E_n} T(e)$$
- **Mejor caso**: Es la cantidad mínima de pasos que toma el algoritmo para una entrada de tamaño $n$.
  $$T(n) = \min_{e \in E_n} T(e)$$
- **Caso promedio**: Es la cantidad promedio de pasos que toma el algoritmo para una entrada de tamaño $n$.
  Esto a su vez depende de una distribución de probabilidad sobre el conjunto de entradas:
  Suponiendo que $p(e)$ es la probabilidad de que la entrada $e$ ocurra, el caso promedio se define como:
  $$T(n) = \sum_{e \in E_n} p(e)\,T(e)$$

A menos que se indique lo contrario, siempre asume que el análisis de un algoritmo se refiere al peor caso: los computólogos son pesimistas por naturaleza.

#### Ejemplo: Ordenamiento por selección

El algoritmo de ordenamiento por selección funciona seleccionando repetidamente el elemento más pequeño de la lista no ordenada y colocándolo en su posición correcta.

```python
def seleccion(L):
    n = len(L) 
    for i in range(n - 1):
        # Sup. que el elemento más pequeño está en la posición i
        min_idx = i
    
        # Busca el elemento más pequeño del resto de la lista
        for j in range(i + 1, n):
            if L[j] < L[min_idx]:
                min_idx = j
        
        # Intercambia el elemento más pequeño encontrado con el elemento en la posición i
        L[i], L[min_idx] = L[min_idx], L[i]
```

Para analizar la complejidad temporal del algoritmo, contamos el número de comparaciones realizadas en el ciclo interno.

1. En la primera iteración del ciclo externo ($i = 0$), el ciclo interno realiza $n - 1$ comparaciones.
2. En la segunda iteración ($i = 1$), el ciclo interno realiza $n - 2$ comparaciones.
3. En general, en la $i$-ésima iteración, el ciclo interno realiza $n - i - 1$ comparaciones.

El número total de comparaciones $T(n)$ realizadas por el algoritmo es la suma de todas las comparaciones realizadas en cada iteración del ciclo externo:

$$
\begin{align*}
T(n) &= \sum_{i=0}^{n-2} (n - i - 1) \\
  &= \sum_{i=0}^{n-2} (n - 1 - i) \\
  &= \sum_{i=0}^{n-2} (n - 1) - \sum_{i=0}^{n-2} i \\
  &= (n - 1)(n - 1) - \frac{(n - 2)(n - 1)}{2} \\
  &= \frac{2(n - 1)(n - 1) - (n - 2)(n - 1)}{2} \\
  &= \frac{(n - 1)(2n - 2 - n + 2)}{2} \\
  &= \frac{(n - 1)(n)}{2}.
\end{align*}
$$

Por lo tanto, el número total de comparaciones es:

$$T(n) = \frac{n(n - 1)}{2}.$$

### Crecimiento asintótico

Quizás te preguntes por qué no se considera el número de intercambios realizados por el algoritmo, o bien el número total de pasos que toma el algoritmo, incluyendo cada comparación, cada suma, cada asignación, etc.
Por ahora nos limitaremos a decir que estas otras operaciones son irrelevantes cuando consideras lo siguiente:

- El número de pasos que toma el algoritmo es proporcional al número de comparaciones realizadas.
- El tiempo exacto que toma el algoritmo depende de la implementación y del hardware utilizado, pero la relación entre el número de pasos y el tiempo es constante.
- La diferencia entre el número exacto de pasos y el número de comparaciones debe ser a lo sumo un factor constante más un término despreciable cuando $n$ es grande.

Llevando esta idea un poco más lejos, podemos formalizar este concepto de que unos términos sean despreciables en comparación con otros usando la notación *asintótica*, mejor conocida como **notación O-grande**.

**Definición** (Notación O-grande): Sea $f(n)$ y $g(n)$ funciones de $\mathbf{N} \to \mathbf{R}$.
Decimos que $f(n)$ es $O(g(n))$ (“$f$ crece no más rápido que $g$”) si existe una constante $c > 0$ tal que $f(n) \leq c\,g(n)$ para todo $n$ salvo un número finito de casos.

Por razones históricas usamos la notación $f(n) = O(g(n))$, donde el signo de igualdad no es del todo correcto.
Una notación más apropiada podría ser $f \le_O g$, pero nadie la usa.

**Ejemplo**: Se desea decidir entre dos algoritmos, $A$ y $B$, que resuelven el mismo problema.

- El algoritmo $A$ tiene una complejidad temporal de $T_A(n) = 2\,n + 20$.
- El algoritmo $B$ tiene una complejidad temporal de $T_B(n) = n^2 + 10\,n + 5$.

La elección depende del tamaño de la entrada $n$.

- Si $n$ es pequeño, $T_B(n)$ es más pequeño que $T_A(n)$; por ejemplo, para $n = 1$, $T_A(1) = 22$ y $T_B(1) = 16$.
- Si $n$ es grande, $T_A(n)$ es más pequeño que $T_B(n)$; por ejemplo, para $n = 100$, $T_A(100) = 220$ y $T_B(100) = 10505$.

Claramente el algoritmo $A$ es mejor conforme $n$ crece, decimos que $A$ *escala mejor que* $B$.
La notación O-grande nos permite expresar esto de manera formal:

$$
T_A(n) = O(T_B(n)).
$$

Para mostrar que $T_A(n) = O(T_B(n))$, debemos encontrar una constante $c > 0$ tal que $T_A(n) \leq c\,T_B(n)$ para todo $n$ salvo un número finito de casos. En este caso, basta con tomar $c = 1$ y $n \geq 20$.

Algunas reglas útiles para la notación O-grande son:

- Las constantes multiplicativas pueden ser ignoradas: $O(c\,f(n)) = O(f(n))$.
- La suma de dos funciones es la función que crece más rápido: $O(f(n) + g(n)) = O(\max(f(n), g(n)))$.
- $n^a$ domina $n^b$ si $a > b$: por ejemplo, $O(n^2 + n) = O(n^2)$.
- Toda función exponencial crece más rápido que cualquier polinómica: $O(n^k) = O(2^n)$.
- De forma similar toda función polinómica crece más rápido que cualquier función logarítmica: $O(\log(n)) = O(n^k)$.

### Ejemplos adicionales de notación O-grande

1. **Simplificación de $3\,n^2\,\log n + 20$**:
   - La constante $20$ es despreciable en comparación con $3\,n^2\,\log n$ para valores grandes de $n$.
   - Por lo tanto, $3\,n^2\,\log n + 20 = O(n^2\,\log n)$.

2. **Comparación de $5\,n^3 + 2\,n^2$**:
   - El término $5\,n^3$ domina $2\,n^2$ para valores grandes de $n$.
   - Por lo tanto, $5\,n^3 + 2\,n^2 = O(n^3)$.

3. **Simplificación de $n^4 + n^3 + n^2 + n$**:
   - El término $n^4$ domina todos los demás términos para valores grandes de $n$.
   - Por lo tanto, $n^4 + n^3 + n^2 + n = O(n^4)$.

4. **Logaritmos en diferentes bases**:
   - Los logaritmos en diferentes bases son equivalentes hasta un factor constante.
   - Por ejemplo, $\log_2(n) = O(\log_{10}(n))$.

5. **Crecimiento exponencial frente a polinómico**:
   - Una función exponencial como $2^n$ crece más rápido que cualquier función polinómica como $n^k$.
   - Por ejemplo, $n^3 + 2^n = O(2^n)$.

6. **Crecimiento logarítmico frente a constante**:
   - Una función logarítmica como $\log(n)$ crece más rápido que una constante.
   - Por ejemplo, $5 + \log(n) = O(\log(n))$.

Estos ejemplos ilustran cómo simplificar expresiones y comparar el crecimiento de diferentes funciones utilizando la notación O-grande.

### Ejemplos de conteo de operaciones con notación O-grande

1. **Ciclo simple**:

   ```python
   for i in range(n):
       # Operación constante
       print(i)
   ```

   - Complejidad: $O(n)$
   - El ciclo se ejecuta $n$ veces, y cada iteración realiza una operación constante.

2. **Ciclo anidado**:

   ```python
   for i in range(m):
       for j in range(n):
           # Operación constante
           print(i, j)
   ```

   - Complejidad: $O(m \cdot n)$
   - El ciclo externo se ejecuta $m$ veces, y por cada iteración del ciclo externo, el ciclo interno se ejecuta $n$ veces.

3. **Ciclo anidado con dependencia**:

   ```python
   for i in range(n):
       for j in range(i):
           # Operación constante
           print(i, j)
   ```

   - Complejidad: $O(n^2)$
   - El ciclo externo se ejecuta $n$ veces, pero el ciclo interno se ejecuta $i$ veces en la $i$-ésima iteración. El número total de iteraciones es:
     $$
     \sum_{i=1}^{n} i = \frac{n(n-1)}{2} = O(n^2)
     $$

4. **Ciclo con crecimiento logarítmico**:

   ```python
   i = 1
   while i < n:
       # Operación constante
       print(i)
       i *= 2
   ```

   - Complejidad: $O(\log(n))$
   - El valor de $i$ se duplica en cada iteración, por lo que el ciclo se ejecuta aproximadamente $\log_2(n)$ veces.

5. **Ciclo triple anidado**:

   ```python
   for i in range(n):
       for j in range(n):
           for k in range(n):
               # Operación constante
               print(i, j, k)
   ```

   - Complejidad: $O(n^3)$
   - Cada ciclo anidado se ejecuta $n$ veces, resultando en $n \cdot n \cdot n = n^3$ iteraciones.

Estos ejemplos muestran cómo analizar y calcular la complejidad temporal de diferentes estructuras de bucles utilizando la notación O-grande.

## 9.3 Estrategias de diseño de algoritmos

... Lo veremos en la siguiente clase.
